{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78780a9-bf15-4eff-ac39-f7f3df75c065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.projectnf.dfs.core.windows.net\",\n",
    "  \"ntzj8hOFCbU6O7L069dR4Bxqnb/uE8v9STC+eqHFitEANjJpSskZE0oW9GI6o3xr0TP5AOagZb0P+AStupAZ8g==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab93eeba-55a9-4e6c-a610-f7fd38f1b174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"abfss://lakehouse@projectnf.dfs.core.windows.net/processed/metadata_csv/\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\n",
    "    \"abfss://lakehouse@projectnf.dfs.core.windows.net/processed/metadata_parquet/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5faf8ad-69e3-4891-99a5-4917e444f0bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+----+\n|            filename|               label|path|size|\n+--------------------+--------------------+----+----+\n|\u0005+\u00007\u0019W\u000471\u0015+\u00007\u0019�\u00007...|                NULL|NULL|NULL|\n|                NULL|9g\u000496\u0015�\u000099\u0010\u000099r\u00149...|NULL|NULL|\n|\u001C\u0015R\u0015\u0000\u0015\u0006\u0015\\b\u0000\u0000�\u00020\u0002\u0000...|                NULL|NULL|NULL|\n|                  J�|                NULL|NULL|NULL|\n|\u00009�>\u0005J>\u0005\u00009�1\\bJ1\\...|                NULL|NULL|NULL|\n+--------------------+--------------------+----+----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "metadata = spark.read.parquet(\n",
    "    \"abfss://lakehouse@projectnf.dfs.core.windows.net/processed/metadata_parquet/\"\n",
    ")\n",
    "\n",
    "metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4e985c-48b0-4966-a6bb-05a09e38c9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+-----+\n|filename|label|                path| size|\n+--------+-----+--------------------+-----+\n| Y52.jpg|  yes|abfss://lakehouse...|61240|\n| Y53.jpg|  yes|abfss://lakehouse...| 5011|\n| Y54.jpg|  yes|abfss://lakehouse...|15874|\n| Y55.jpg|  yes|abfss://lakehouse...|43087|\n| Y56.jpg|  yes|abfss://lakehouse...| 7633|\n+--------+-----+--------------------+-----+\nonly showing top 5 rows\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = metadata\n",
    "\n",
    "# 1. Remove rows with missing important fields\n",
    "df = df.filter(\n",
    "    col(\"filename\").isNotNull() &\n",
    "    col(\"label\").isNotNull() &\n",
    "    col(\"path\").isNotNull()\n",
    ")\n",
    "\n",
    "# 2. Remove images with zero or negative size\n",
    "df = df.filter(\n",
    "    col(\"size\").cast(\"int\") > 0\n",
    ")\n",
    "\n",
    "df.show(5)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff406b64-260d-49ca-89c1-b56f26e3bb28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"label_index\",\n",
    "    when(col(\"label\") == \"yes\", 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9beebc2b-4ca2-4774-8f53-957d33c39000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"abfss://lakehouse@projectnf.dfs.core.windows.net/gold/brain_metadata_clean/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556679f7-de80-4cdd-9fc7-fe85bbcde493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"abfss://lakehouse@projectnf.dfs.core.windows.net/gold/brain_metadata_clean/\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "brain_metadata_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}